---
title: "bayeshmm"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## R Markdown

This is an R Markdown presentation. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.

## Slide with Bullets

- Bullet 1
- Bullet 2
- Bullet 3

## Slide with R Output

```{r cars, echo = TRUE}
summary(cars)
```

## Slide with Plot

```{r pressure}
plot(pressure)
```

## 注意

* 「ベイズ推論による機械学習入門」の一部を切り取った解説です

* それを飛び越えた難しい話はしません/できません

# 第一部：変分ベイズ
まずここから

## KLダイバージェンスの定義

最初にKLダイバージェンスの定義を見直す。

$$
\displaystyle
KL(q||p) :=\int_{S(q)}q(x) \ln \frac{p}{q}dx =\left\langle \ln \frac{p}{q} \right\rangle_{q}
$$
こいつは、$q(x)$から見て$p(x)$がどれくらい近いかを見ている。

## 確率分布の近似

KLについて一般的な事項は以下。

* 対称ではない
* 非負の値をとる
* 分布pとqが互いに同値であれば、$KL(q||p)=0$が成立

ということは、KLが小さくなるような分布を見つければ、何らかの意味で近似になっていると考えることができる。

> ただし、対称的でないところが効いていて、どっちから近似するかで解が異なる。

## 確率分布の近似2

例えばPRML10章では、次の2者を比較している。

* 多峰分布pを**単峰分布qから見た近さ**で単峰分布pで近似
  * $KL(q||p)$をqについて最小化
* 多峰分布pを**多峰分布pから見た近さ**で単峰分布pで近似
  * $KL(p||q)$をqについて最小化

前者の方が、一つの峰を当てに行っている感じが出ており、局所解に辿り着けそうなことを示唆している。

## 変分推論の話
変分ベイズは平均場近似で事後分布を近似する方法。
平均場近似が何者かというのは、物理屋さんに聞くと教えてくれる。
今回は天下り的に導入する。

要は事後分布が欲しいのだけど、それを荒く近似してしまおうというもの。

この「荒く」という部分がポイントで、「独立だと思おう」と仮定を入れる。

## 変分推論の式変形

で、式(4.17)から始まる変形の行間を埋めていく。
とは言っても文中に書いてあるのでそこまで苦労はない。

ポイントは先にも書いたように、いらない計算はconstに積極的に押し込めること。

## 極小値を与える解

ここでKLダイバージェンスを見ると、分布$q$に対して$\mathbb{R}$を返すもので、これを汎関数という。こいつは制約付き最小化と見ることができる。

ということで、結果的に$\ln q = \langle \ln p(1,2,3) \rangle_{2,3} +\mathrm{const}$となる。

# ポアソン混合モデル

## モデル式

ポアソン混合モデルの同時分布は以下で書ける。太字にしてないけど心の目で補って欲しい

$$
p(X,S,\lambda,\pi)=p(X|S,\lambda)p(S|\pi)p(\lambda)p(\pi)
$$

## 変分近似の準備

今回、潜在変数$S$とパラメータ$(\lambda,\pi)$を独立にしたい気持ちのもとで近似する。

ということで、次のようにして二つの事後分布を計算する。

$$
\ln q(S) = \left\langle \ln p(X,S,\lambda,\pi) \right\rangle_{q(\lambda,\pi)}
$$

$$
\ln q(\lambda,\pi) = \left\langle \ln p(X,S,\lambda,\pi) \right\rangle_{q(S)}
$$


## 潜在変数の近似計算１
最初に前者の分布を考える。

まずモデル式があるので、このまま分解してしまう。(4.47)の変形におけるポイントは、分解した後、$\lambda$や$\pi$を持たない集団とか、同時に含んでいないのでいい感じに処理できること

次に最後の行の第一項、これは(4.28)式と4.34式から次のようになる。

さらに第二項、これはなんかいい感じに計算すればよい、


## パラメータの近似計算１

(4.52)式は、(4.5)をそのまま適用し、いい感じに分解できる。

このとき、lnの上で項が綺麗に分割できていると、expをとると積に別れ、つまり独立な分布として考えて良いことを利用し、もっと計算がやりやすくなる。
(独立の定義を思い出してから、対数をとってみよう。)

ここまでくれば、あとは期待値取るだけでOK

# 隠れマルコフモデル

## モデル式
1期前のデータから遷移する、というのをきちんと書くと、モデルは(5.76)のようになる。

$$
p(X,S,\lambda,\pi,A)=P(X|\lambda)p(S|\pi,A) p(\lambda)p(\pi)p(A)
$$
もう少し書き下したものが以下

$$
\displaystyle
p(\lambda)p(\pi)p(A)p(x_1 | s_1 ,\lambda) p(s_1|\pi) \prod_{n=2}^{N}p(x_n|s_n,\lambda) p(s_n|s_{n-1}|A)
$$

これを変分推論で考える。

## 完全分解変文推論

これは書籍の結果を素直に受け入れることにする、理由は後述。

事後分布の分解の方針は(5.79)、本当に全部バラバラにする。

必要なのは(5.86)、(5.90)、(5.93)、(5.105)〜(5.108)あたりを天下り的に受け入れてしまえばよい。


## 構造化変分推論

(5.79)はバラバラにしすぎてて若干のやりすぎ感がある。なので$S$と$\lambda,A,\pi$で分離する。

さて、完全分解変分推論と構造化変分推論の違いは、潜在変数$S$に関する分解の違いのみ。
この辺を意識して変分近似の式変形を行うと、パラメータ更新式は構造化変分推論のものがそのまま使えることになる。(興味があれば質問してください、メモはとってあるので板書します。)


## 頑張って計算したいもの

唯一異なる点は、p187中ほどにあるように、潜在変数の期待値を再計算する必要があること。このためにフォワードバックワードアルゴリズムで「時点ごと」$\times$「潜在クラスごと」にqを計算する。

$$
\displaystyle
q(s_n) \propto f(s_n) b(s_n) 
$$

fがフォワードな方でbがバックワードな方。それぞれの概形は書籍(5.122)と(5.123)を参照のこと。


## フォワード

(5.122)の通りなのだが、こいつを実際に計算するためには(5.116)と(5.115)が必要。
中身の期待値はそれぞれ(5.95)と(5.74)を見れば良い。
(5.74)は期待値になってないけど、これ中身は$K\times K$の遷移行列で、列ごとに遷移確率があるだけなので期待値として要素の値ががそのまま使える。(ただし書籍にもあるように列ごとに正規化しておく必要はある)


## バックワード

こちらも(5.123)の通り。forwardと同様に(5.116)や(5.115)を使って計算する。

## sだけ

先程のforwardとbackwardを頑張って計算すれば、$q(s_n) \propto f(s_n) b(s_n) $なのであとはスカラー積を計算するだけ。

## ssの部分

式は(5.121)なので楽、のように見えるが実はちょっとだけめんどくさい。

ある時点nについて考える。f、bはベクトル、$p(s_n|s_{n-1})$は行列、$p(x_n|s_n)$はベクトルであり、さらに求めたいのは行列になるので、どうやって計算したらいいのかわかんないと思う。

ということでfとbを列ベクトルだと思うと、アダマール積なんかを使っていい感じに書ける。

（カンペ：ここで講師が図形的に華麗な板書を行う。）

# stanで書く方のHMM

## stanの書き方

正直、これはウェブに素晴らしい解説がわんさかあるので省略、基本的には**考えた生成過程に従ってひたすら書く**だけでよい。

なお、ベクトル化など早い書き方もあるのだけど、今回は「とりあえず書いてみるstan」くらいの立ち位置なので割愛する。

## モデル

はっきり言ってそんなに難しくない。実はstanのtutorialにあり、尤度が書ければOK。

